\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Exploring market power using deep reinforcement learning for intelligent bidding strategies\\
\thanks{EPSRC}
}

\author{\IEEEauthorblockN{Alexander J. M. Kell, Matthew Forshaw, A. Stephen McGough}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Newcastle University}\\
Newcastle upon Tyne, U.K. \\
\{a.kell2, matthew.forshaw, stephen.mcgough\}@newcastle.ac.uk}
%\and
%\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
}

\maketitle

\begin{abstract}

Abstract goes here.

% Background

% Methodology

% Results



\end{abstract}

\begin{IEEEkeywords}
reinforcement learning, bidding strategy, multi-agent system, electricity markets
\end{IEEEkeywords}

\section{Introduction}

Under perfectly competitive electricity markets, generator companies (GenCos) tend to bid their short-run marginal costs (SRMC) when bidding into the day-ahead electricity market. Where the SRMC is the cost to produce a single MWh of electricity, and excludes capital costs. However, electricity markets are often oligopolistic, where a small subset of GenCos provide a majority of the capacity to the market. Under these conditions, it is possible that the assumption that GenCos are price-takers do not hold. That is, large GenCos artificially increase the price of electricity to gain increasing profit using their market power. 

Reduced competition within electricity markets can lead to higher prices to the consumers, for no increased benefit. It is therefore within the interests of the consumer and that of government to maintain a competitive market. Competition within electricity markets can be decreased through having a single, or multiple entities with large control over capacity in electricity markets. 

In this paper, we explore the effect of total controlled capacity on electricity prices. Specifically, we use deep reinforcement learning (RL) to calculate a bidding strategy for GenCos in a day-ahead market. These GenCos are modelled as agents within the agent-based model, ElecSim \cite{Kell}. We use the UK electricity market instantiated in 2018 as a case study, similar to our work in \cite{Kell2019a}. That is, we model each GenCo with their respective power plants in the year 2018 to 2019. In total we model 60 GenCos with 1085 power plants.

We use the deep deterministic policy gradient deep RL algorithm, which allows for a continuous action space \cite{Hunt2016}. Conventional RL methods require discretization of state and/or action spaces and therefore suffer from the curse of dimensionality \cite{Ye2020a}. As the number of discrete states and actions increases, the computational cost grows exponentially. However, too small a number of discrete states and actions will reduce the information available to the GenCos, leading to sub-optimal bidding strategies. Additionally, by using a continuous approach, we allow for GenCos to consider increasingly complex bidding strategies. 

Other work considers a simplified model of an electricity market by modelling a small number of GenCos or plants \cite{EsmaeiliAliabadi2017}. We, however, model each GenCo as per the UK electricity market with their respective power plants in a day-ahead market. Additionally, other work focuses on a bidding strategy to maximize profit for a GenCo. In our work, we focus on the impact of large GenCos, as well as collusion between GenCos on total the electricity price. 


Our approach does not require GenCos to formulate any knowledge of the information informing the market clearing algorithm or rival GenCo bidding strategies. This enables a more realistic simulation


%- Why the current approaches / systems don't solve the problem


%- What is the innovation in this work



%- A short description of the solution



%- What are the key take-home messages



% Contributions of this work

- Look at market power of individual or groups of agents

%- Outline of the rest of the work




\section{Literature Review}

- Game theory versus agent based models\\

- Application of Erev Roth, Q-learning to markets.\\

- Applications of reinforcement learning to bidding strategies \\
- Reinforcement learning in energy markets \\


\section{Material}

- Market Structure of ElecSim (yearly outlook) \\
- Real life structure of the UK\\
- Assumed that all capacity is bid\\
- We change the price of the bid (from SRMC to Bid cap in the market)\\
- The GenCo has no information about the generation capacity, marginal cost, bid prices or profits of other GenCos, or the total number of GenCos in the system. \cite{EsmaeiliAliabadi2017}\\

- Do not consider flow constraints due to the large nature of the simulation\\
- Amount of time taken to run 1 episode is...\\ 


- Introduction to RL and DDPG (model-free approach and continuous action space) \\
- Problems with Q-learning not using entire continuous space. \\
- Explore our parameters of DDPG

-Note that in the model, the GenCo does not take any strategic action, that is, the GenCo does not consider the actions of other GenCos explicitly in its decision process. In fact, it does not have information on other GenCos. The GenCo is modeled as a simple agent that learns only from its own experi- ence. GenCosâ€™ collective behavior, however, may lead to strategic outcomes.\cite{EsmaeiliAliabadi2017}\\


\section{Methodology}

- Grouping agents based upon size and seeing results\\
- Observation and action space \\
- Allowing them to bid maximum of \textsterling600 and a market cap of \textsterling150



\section{Results}

- Show time-steps vs. reward for both scenarios \\
- Show the step change in reward after a certain amount of controlled capacity

\section{Discussion}

- Make suggestions based upon optimal level of competition \\
- Importance of understanding market power and having a regulator otherwise prices can significantly increase

\section{Conclusion}

- Future work (withold capacity)


\section{Acknowledgment}

This work was supported by the Engineering and Physical Sciences Research Council, Centre for Doctoral Training in Cloud Computing for Big Data [grant number EP/L015358/1].


%\section*{References}

\bibliographystyle{IEEEtran}
\bibliography{library,custom_bib}


\end{document}
